<!doctype html><html>
<head>
<title>Reinforcement learning, lesson 1</title>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="ie=edge">
<link rel=stylesheet href=/css/bootstrap.min.css>
<link rel=stylesheet href=/css/layouts/main.css>
<link rel=stylesheet href=/css/navigators/navbar.css>
<link rel=stylesheet href=/css/plyr.css>
<link rel=stylesheet href=/css/flag-icon.min.css>
<link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css>
<link rel=icon type=image/png href=/images/site/favicon3_hu62e62282d1df9a6f522c194700ff0a75_33785_42x0_resize_box_3.png>
<meta property="og:title" content="Reinforcement learning, lesson 1">
<meta property="og:description" content="Sample post with multiple images, embedded video ect.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://arthurzucker.github.io/posts/mva/rl/lesson1/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-10-18T00:00:00+00:00">
<meta property="article:modified_time" content="2021-10-18T00:00:00+00:00">
<meta name=description content="Sample post with multiple images, embedded video ect.">
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css>
<link rel=stylesheet href=/css/layouts/single.css>
<link rel=stylesheet href=/css/navigators/sidebar.css>
<link rel=stylesheet href=/css/style.css>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YRJ694EF5C"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-YRJ694EF5C',{anonymize_ip:!1})}</script>
</head>
<body data-spy=scroll data-target=#TableOfContents data-offset=80>
<div class="container-fluid bg-dimmed wrapper">
<nav class="navbar navbar-expand-xl top-navbar final-navbar shadow">
<div class=container>
<button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/>
<img src=/images/site/favicon3_hu62e62282d1df9a6f522c194700ff0a75_33785_42x0_resize_box_3.png alt=Logo>
Arthur Zucker's Portfolio</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse lang-selector" id=top-nav-items>
<ul class="navbar-nav ml-auto">
</ul>
</div>
</div>
<img src=/images/site/favicon3_hu62e62282d1df9a6f522c194700ff0a75_33785_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/favicon4_hu5db1c7af89e12ca8c711bcc33bb3007c_15810_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo">
</nav>
<section class=sidebar-section id=sidebar-section>
<div class=sidebar-holder>
<div class=sidebar id=sidebar>
<form class=mx-auto method=get action=/search>
<input type=text name=keyword placeholder=Search data-search id=search-box>
</form>
<div class=sidebar-tree>
<ul class=tree id=tree>
<li id=list-heading><a href=/posts data-filter=all>Posts</a></li>
<div class=subtree>
<li>
<i class="fas fa-plus-circle"></i><a href=/posts/flying-foxes-study/>Passive Acoustic Monitoring Using AI</a>
<ul>
<li>
<i class="fas fa-plus-circle"></i><a href=/posts/flying-foxes-study/ai/>Deep Learning pipeline</a>
<ul>
<li><a href=/posts/flying-foxes-study/ai/aed/ title="Audio Event Detection">Audio Event Detection</a></li>
<li><a href=/posts/flying-foxes-study/ai/speaker/ title="Speaker Recognition">Speaker Recognition</a></li>
</ul>
</li>
<li><a href=/posts/flying-foxes-study/ecology/ title=Ecology>Ecology</a></li>
</ul>
</li>
<li>
<i class="fas fa-plus-circle"></i><a href=/posts/projects/>Projects</a>
<ul>
<li><a href=/posts/projects/safran/ title="Semantic Segmentation for cars">Semantic Segmentation for cars</a></li>
<li><a href=/posts/projects/clusti/ title="Table Structure Recognition">Table Structure Recognition</a></li>
</ul>
</li>
<li>
<i class="fas fa-minus-circle"></i><a class=active href=/posts/mva/>MVA Courses</a>
<ul class=active>
<li>
<i class="fas fa-plus-circle"></i><a href=/posts/mva/recvis/>Computer Vision & Object Recognition</a>
<ul>
<li><a href=/posts/mva/recvis/lesson1/ title="Lesson 1">Lesson 1</a></li>
<li><a href=/posts/mva/recvis/lesson3/ title="Lesson 3">Lesson 3</a></li>
</ul>
</li>
<li>
<i class="fas fa-plus-circle"></i><a href=/posts/mva/deeplearning/>Deep Learning</a>
<ul>
<li><a href=/posts/mva/deeplearning/lesson2/ title="Lesson 2">Lesson 2</a></li>
</ul>
</li>
<li>
<i class="fas fa-minus-circle"></i><a class=active href=/posts/mva/rl/>Reinforcement Learning</a>
<ul class=active>
<li><a class=active href=/posts/mva/rl/lesson1/ title="Lesson 1">Lesson 1</a></li>
<li><a href=/posts/mva/rl/lesson2/ title="Lesson 2">Lesson 2</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=/posts/backward/ title="Backward propagation">Backward propagation</a></li>
</div>
</ul>
</div>
</div>
</div>
</section>
<section class=content-section id=content-section>
<div class=content>
<div class="container p-0 read-area">
<div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://arthurzucker.github.io/posts/mva/rl/lesson1/images/forest.jpg)>
</div>
<div class=page-content>
<div class="author-profile ml-auto align-self-lg-center">
<img class=rounded-circle src=/images/author/me_hue0c31f5a221a4dac786736a317c9bf63_45873_120x120_fit_q75_box.jpg alt="Author Image">
<h5 class=author-name>Arthur Zucker</h5>
<p>October 18, 2021</p>
</div>
<div class=title>
<h1>Reinforcement learning, lesson 1</h1>
</div>
<div class=taxonomy-terms>
<ul>
<li class=rounded><a href=/tags/mva class="btn, btn-sm">MVA</a></li>
<li class=rounded><a href=/tags/reinforcement-learning class="btn, btn-sm">Reinforcement Learning</a></li>
<li class=rounded><a href=/tags/english class="btn, btn-sm">English</a></li>
</ul>
</div>
<div class=post-content id=post-content>
<style>r{color:Red}o{color:Orange}g{color:Green}b{color:Blue}</style>
<h2 id=introduction>Introduction</h2>
<h3 id=basic-rl-model>Basic RL model</h3>
<p>In any RL model, there is :</p>
<ul>
<li>an RL agent</li>
<li>an Environment, deterministic or stochastic</li>
<li>actions</li>
<li>rewards ( long/short term, intermediate)</li>
<li>states (known/unknown)</li>
<li>a goal: maximizing the reward function</li>
</ul>
<div class="alert alert-success">
<strong>RL is a <strong>Framework for learning by interaction under uncertainty</strong>. An agent takes actions, receives rewards, and the environment evolves to the next state.</strong>
</div>
<h2 id=1-markov-decision-process>1. Markov Decision Process</h2>
<p>A RL problem is modelled using <em>Markov Decision Processes</em>.</p>
<h3 id=the-rl-interactions>The RL interactions</h3>
<p>The classic RL interaction is as follows:</p>
<p>We have a discrete decision time $t$ (which would be for example seconds), and at each timestamp :</p>
<ol>
<li>The <em>agent</em> selects an <em>action</em> $a_t$, based on $s_t$ the current <em>state</em> of the system.</li>
<li>The <em>agent</em> gets his <em>reward</em> $r_t$</li>
<li>The <em>Environment</em> moves to a new state $s_{t+1}$</li>
</ol>
<h3 id=-markov-chains>üìù Markov chains</h3>
<p>Markov Chains are the building blocks of RL. A Markov Chain is a stochastic (which means that it includes randomness ) model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. In order to create a model, we need to introduce mathematical definitions.</p>
<div class="alert alert-success">
<strong><p><g>Definition :</g> Given $S$, a state space and a bounded compact subset of the Euclidean space, <strong>the discrete time dynamic system $(s_t)_{t\in \mathbb{N}}$</strong> is a markov Chain <strong>if it satisfies</strong> the <r>Markov property</r>:</p>
<p>$$ \mathbb{P}\left(s_{t+1}=s \mid s_{t}, s_{t-1}, \ldots, s_{0}\right)=\mathbb{P}\left(s_{t+1}=s \mid s_{t}\right)$$</p>
</strong>
</div>
<h3 id=-markov-decision-process-mdp>üìù Markov Decision Process (MDP)</h3>
<p>A Markov Decision Process is a tool, to model decision making when outcomes can be random.</p>
<div class="alert alert-success">
<strong><p><g>Definition :</g> a Markov Decision Process is defined as a tuple <b>$M=(S,A,p,r)$</b> with</p>
<ul>
<li>$S$ the (finite) State space</li>
<li>$A$ the (finite) Action space</li>
<li>$p(s'|s,a)$ the transition probability, such that $\mathbb{P}\left(s_{t+1}=s' \mid s_{t}=s,a_t=a\right)$</li>
<li>$r(s,a,s')$ is the reward of the transition from $s$ to $s'$ taking the action $a$. The reward can be stochastic, but we can express its distribution, $v$ for $s,a$ as $v(s,a)$. Thus the reward can be writtent as : $r(s,a) = \mathbb{E}_{R\sim v(s,a)}[R]$.</li>
</ul>
</strong>
</div>
<h4 id=assumptions>Assumptions</h4>
<p><b>Markov assumptions</b> : the current state $s$ and the action $a$ are a sufficient statistic for the next state $s'$. This means that &ldquo;no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter&rdquo;<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. <strong>Our statistic provides as much information as possible.</strong></p>
<p><b>Time assumption</b> : discrete time, at each step, $t\leftarrow t+1$.</p>
<p><b>Reward assumption</b> : the reward is uniquely defined by a transition from a state $s$, to $s'$ and an action $a$.</p>
<p><b>Stationary assumption</b> : transition and reward don&rsquo;t change with time. Otherwise, it&rsquo;s non-stationary.</p>
<h2 id=2-policy>2. Policy</h2>
<div class="alert alert-success">
<strong><p><g>Definition : </g>a decision rule $d$, which chooses an action from the state $S$ of the system can be :</p>
<ul>
<li>Deterministic, given the same set of state $S$, the action $a\in A$ will be chosen</li>
<li>Stochastic, a probability distribution over the actions determines $a$.</li>
</ul>
<p>Then, it can also be :</p>
<ul>
<li>( stochastic / deterministic ) History-dependant</li>
<li>( stochastic / deterministic ) Markov (relies on the previous state)</li>
</ul>
</strong>
</div>
<div class="alert alert-success">
<strong><p><g>Definition</g> : A policy is a sequence of decision rules, it can be :</p>
<ul>
<li>Non-stationary, then $\pi = (d_0,d_1,&mldr;)$</li>
<li>stationary, then $\pi = (d,d,&mldr;)$</li>
</ul>
</strong>
</div>
<p>At each round t, an agent following the policy $\pi$ selects the actions $a_t \sim d_t(s_t)$ .</p>
<h3 id=markov-chain-of-a-policy>Markov chain of a policy</h3>
<p>The transition probability of a random process $(s_t)_{t \in \mathbb{N}}$ with regards to a stationary policy $\pi$ is the following :</p>
<p>$$ p^\pi (s'\mid s) = \mathbb{P}(s_{t+1}=s' \mid s_t = s, \pi) $$</p>
<p>Every actions in $A$ (even if it does not take the state from $s$ to $s'$) has to be taken into account. Thus we deduce that $$ p^\pi (s'\mid s) = \sum_{a\in A} \pi(s,a)p(s'\mid s,a) $$</p>
<h2 id=3-optimality-principle>3. Optimality Principle</h2>
<p>How do we evaluate a policy ?</p>
<h3 id=-state-value-function>üìù State value function</h3>
<p>The state value function allows us to determine the effectiveness of a policy. Various definitions can be used depending on the problem at hand. For the following definitions, let $\pi = (d_1, d_2, &mldr;,)$ be a deterministic policy.</p>
<div style=padding-left:30px>
üé≤ The <g>Finite time horizon T</g> policy, sets a deadline $T$, at which the sum of the rewards up to $T$ will be computed. The value function can be expressed as :
<p>$$ V^\pi(t,s) = \mathbb{E}\left[ \sum_{\tau = t}^{T-1} r(s_\tau,d_\tau(h_\tau)) + R(s_T) \mid s_t = s; \pi = (d_1, d_2, &mldr;,) \right ]$$</p>
<p>Here, where $R$ is a value function for the final state. Here we compute the expectation of the sum of the rewards obtained taking actions according to the policy, and receiving the corresponding rewards. We have to take into account the Expectation because the decision process can be stochastic, and thus the Expectation gives the equivalent of the mean rewards in a stochastic model. <br>
$\qquad$üìù It is usually used when there is a deadline to meet.</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Infinite Time Horizon with discount </g> translates a problem that never terminates. Rewards closer in time receive a higher importance :
<p>$$ V^{\pi}\left(s\right)\ =\ \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^tr\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi \right] $$</p>
<p>where $0\leq\gamma&lt;1$ is the <em>discount</em> factor. <br>
$\qquad$üìù We implement it when there is uncertainty about the deadline, or intrinsic definition of discount</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Stochastic shortest path </g>, the problem never terminates but the agent will reach a termination state.
<p>$$V^{\pi}\left(s\right)\ =\ \mathbb{E}\left[\sum_{t=0}^{T_{\pi}}r\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi\right]$$</p>
<p>$T_{\pi}$ : first random time when the termination state is achieved. <br>
$\qquad$üìù Used when there is a specific goal condition</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Infinite time horizon with average reward</g>, the problem never terminates but the agent focuses on the expected average of the rewards
<p>$$V^{\pi}\left(s\right)\ =\ \lim_{T\to\infty}\ \mathbb{E}\left[\frac{1}{T}\sum_{t=0}^{T-1}r\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi\right]$$</p>
<p>$T_{\pi}$ : first random time when the termination state is achieved. <br>
$\qquad$üìù Used when the system should be constantly controlled over tim</p>
</div>
<h3 id=optimization-problem->Optimization problem :</h3>
<div class="alert alert-success">
<strong><g>Definition : </g>The solution to an MDP is an optimal policy $\pi^<em>$ satisfying $\pi^</em> \in \underset{\pi \in \Pi}{\text{argmax }} V^\pi$</strong>
</div>
<p>The value function under that policy is the optimal value function : $V^* = V^{\pi^*}$</p>
<h2 id=-application->üéÆ Application :</h2>
<p>I will use a concrete and fun example of a game, where an RL agent can learn to win. The agent will play the <a href=https://www.codingame.com/ide/puzzle/spring-challenge-2021>2021 Coding Game Spring Challenge</a>, the Photosynthesis game (but a bit simplified)</p>
<img src=images/game.png alt="Photosynthesis board game" class=center>
<p>Here, the environment is the board game. Let&rsquo;s define the various notions that we introduced before in this framework.</p>
<div style=padding-left:30px>
üéØ The <g>goal</g> is to end the game with more points than your opponent. For now, let's just say that the goal is to gain as much <e>sun</e> ‚òÄÔ∏è as possible.
</div>
<div style=padding-left:30px>
üå≥ The <g>environment</g> in which the game takes place is a forest.
</div>
<div style=padding-left:30px>
üí≠ The <g>actions</g> available to the agent are the following :
<ul>
<li>SEED : Planting a seed. It can only be done in the range of the size of the tree.</li>
<li>GROW : Growing a tree. From a seed (size 0) to a sprout (size 1) to a sapling (size 2) to a mature tree (size 3)</li>
<li>CUT : Cut a tree. Only mature trees can be cut</li>
<li>WAIT : Do nothing</li>
</ul>
</div>
<div style=padding-left:30px>
üí∞ Each action has a <g>reward</g> associated:
<ul>
<li>Planting a seed costs <r>1 sun</r> plus <code>number of seeds already on the board</code>-suns. (for example, in the figure above, planting a seed would cost 3 suns)</li>
<li>Growing a tree cost <code>size_of_the_tree</code>-suns plus <code>number of trees withe the same size on the board</code>-suns.</li>
<li>Cutting a tree gives a <g>reward</g> of <g>3 suns</g>. (let&rsquo;s forget for now the quality of the soil)</li>
<li>Do nothing</li>
</ul>
</div>
<div style=padding-left:30px>
üìç The game <g>state</g> is defined by the current state of the board.
</div><section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p><a href=https://en.wikipedia.org/wiki/Markov_chain>https://en.wikipedia.org/wiki/Markov_chain</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p><a href=https://en.wikipedia.org/wiki/Sufficient_statistic#cite_note-Fisher1922-1>https://en.wikipedia.org/wiki/Sufficient_statistic#cite_note-Fisher1922-1</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
<div class="row pl-3 pr-3">
<div class="col-md-6 share-buttons">
<strong>Share on:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f" target=_blank>
<i class="fab fa-facebook"></i>
</a>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f&text=Reinforcement%20learning%2c%20lesson%201&via=Arthur%20Zucker%27s%20Portfolio" target=_blank>
<i class="fab fa-twitter"></i>
</a>
<a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f&title=Reinforcement%20learning%2c%20lesson%201" target=_blank>
<i class="fab fa-reddit"></i>
</a>
<a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f&title=Reinforcement%20learning%2c%20lesson%201" target=_blank>
<i class="fab fa-linkedin"></i>
</a>
<a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Reinforcement%20learning%2c%20lesson%201 https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f" target=_blank>
<i class="fab fa-whatsapp"></i>
</a>
<a class="btn btn-sm email-btn" href="mailto:?subject=Reinforcement%20learning%2c%20lesson%201&body=https%3a%2f%2farthurzucker.github.io%2fposts%2fmva%2frl%2flesson1%2f" target=_blank>
<i class="fas fa-envelope-open-text"></i>
</a>
</div>
<div class="col-md-6 btn-improve-page">
<a href=https://github.com/ArthurZucker/ArthurZucker.github.io/edit/main/content/posts/MVA/RL/lesson1/index.md title="Improve this page" target=_blank rel=noopener>
<i class="fas fa-code-branch"></i>
Improve this page
</a>
</div>
</div>
<hr>
<div class="row next-prev-navigator">
<div class="col-md-6 previous-article">
<a href=/posts/mva/deeplearning/lesson2/ title="Deep Learning, lesson 1" class="btn btn-outline-info">
<div><i class="fas fa-chevron-circle-left"></i> Prev</div>
<div class=next-prev-text>Deep Learning, lesson 1</div>
</a>
</div>
<div class="col-md-6 next-article">
<a href=/posts/mva/rl/lesson2/ title="Reinforcement learning, lesson 2" class="btn btn-outline-info">
<div>Next <i class="fas fa-chevron-circle-right"></i></div>
<div class=next-prev-text>Reinforcement learning, lesson 2</div>
</a>
</div>
</div>
<hr>
</div>
</div>
</div>
<a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a>
</section>
<section class=toc-section id=toc-section>
<div class=toc-holder>
<h5 class="text-center pl-3">Table of Contents</h5>
<hr>
<div class=toc>
<nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction</a>
<ul>
<li><a href=#basic-rl-model>Basic RL model</a></li>
</ul>
</li>
<li><a href=#1-markov-decision-process>1. Markov Decision Process</a>
<ul>
<li><a href=#the-rl-interactions>The RL interactions</a></li>
<li><a href=#-markov-chains>üìù Markov chains</a></li>
<li><a href=#-markov-decision-process-mdp>üìù Markov Decision Process (MDP)</a>
<ul>
<li><a href=#assumptions>Assumptions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#2-policy>2. Policy</a>
<ul>
<li><a href=#markov-chain-of-a-policy>Markov chain of a policy</a></li>
</ul>
</li>
<li><a href=#3-optimality-principle>3. Optimality Principle</a>
<ul>
<li><a href=#-state-value-function>üìù State value function</a></li>
<li><a href=#optimization-problem->Optimization problem :</a></li>
</ul>
</li>
<li><a href=#-application->üéÆ Application :</a></li>
</ul>
</nav>
</div>
</div>
</section>
</div>
<footer class="container-fluid text-center align-content-center footer pb-2">
<div class="container pt-5">
<div class="row text-left">
<div class="col-md-4 col-sm-12">
<h5>Navigation</h5>
<ul>
<li class=nav-item>
<a class=smooth-scroll href=/#about>About</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#skills>Skills</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#experiences>Experiences</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#education>Education</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#projects>Projects</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#recent-posts>Recent Posts</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#accomplishments>Accomplishments</a>
</li>
<li class=nav-item>
<a class=smooth-scroll href=/#achievements>Achievements</a>
</li>
</ul>
</div>
<div class="col-md-4 col-sm-12">
<h5>Contact me:</h5>
<ul>
<li><span>Email: </span> <span>arthur.zucker@ens-paris-saclay.fr</span></li>
</ul>
</div>
</div>
</div>
</footer>
<script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script>
<script>window.MathJax={tex:{inlineMath:[['$','$']]},options:{enableMenu:!0}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
<style>mjx-container.MathJax[display=inline]{font-size:150%!important}mjx-container.MathJax[display=true]{text-align:left!important}@media screen and (min-width:600px){mjx-container.MathJax[display=true]{padding-left:2rem}}</style>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script>
</body>
</html>