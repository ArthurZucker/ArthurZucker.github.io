<!doctype html><html lang=en data-figures class=page data-mode=lit>
<head>
<title>[MVA] RL : Reinforcement Learning, how to model an RL problem: Markov Decision Processes | Life as a young engineer</title>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.88.1">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PFYJHY3R2Z"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-PFYJHY3R2Z')</script>
<meta property="og:locale" content="en">
<meta property="og:type" content="article">
<meta name=description content="My note for lesson 1 of MOOC course: Reinforcement learning">
<meta name=twitter:card content="summary">
<meta name=twitter:creator content="@none">
<meta name=twitter:title content="[MVA] RL : Reinforcement Learning, how to model an RL problem: Markov Decision Processes">
<meta property="og:url" content="https://arthurzucker.github.io/notes/rl/lecture1/">
<meta property="og:title" content="[MVA] RL : Reinforcement Learning, how to model an RL problem: Markov Decision Processes">
<meta property="og:description" content="My note for lesson 1 of MOOC course: Reinforcement learning">
<meta property="og:image" content="https://arthurzucker.github.io/notes/rl/learning.png">
<link rel=apple-touch-icon sizes=180x180 href=https://arthurzucker.github.io/icons/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://arthurzucker.github.io/icons/favicon-32x32.png>
<link rel=manifest href=https://arthurzucker.github.io/icons/site.webmanifest>
<link rel=mask-icon href=https://arthurzucker.github.io/safari-pinned-tab.svg color=#002538>
<meta name=msapplication-TileColor content="#002538">
<meta name=theme-color content="#002538">
<link rel=canonical href=https://arthurzucker.github.io/notes/rl/lecture1/>
<link rel=preload href=https://arthurzucker.github.io/css/styles.css integrity as=style crossorigin=anonymous>
<link rel=preload href=https://arthurzucker.github.io/js/bundle.min.fad5deef9fb8788694f791bf960a39ad1fa85ea9930819f679df19a20785d9d47d3dccc519e3848753179c86f3f6fe36d06f267539ab55d4104e92fdd99d6962.js as=script integrity="sha512-+tXe75+4eIaU95G/lgo5rR+oXqmTCBn2ed8ZogeF2dR9PczFGeOEh1MXnIbz9v420G8mdTmrVdQQTpL92Z1pYg==" crossorigin=anonymous>
<link rel=stylesheet type=text/css href=https://arthurzucker.github.io/css/styles.css integrity crossorigin=anonymous><link rel=stylesheet href=/css/custom.css>
</head>
<body data-code=100 data-lines=true id=documentTop>
<header id=top-navbar class=nav_header>
<nav class=nav>
<a href=https://arthurzucker.github.io/ class="nav_brand nav_item">
<img alt="Life as a young engineer" src=https://arthurzucker.github.io/logo.png class=logo>
<span class=site-title>Life as a young engineer</span>
<div class=nav_close>
<div><svg class="icon"><use xlink:href="#open-menu"/></svg><svg class="icon"><use xlink:href="#closeme"/></svg>
</div>
</div>
</a>
<div class="nav_body nav_body_right">
<div class=nav_parent>
<a href=https://arthurzucker.github.io/ class=nav_item>Blog </a>
</div>
<div class=nav_parent>
<a href=https://arthurzucker.github.io/projects/ class=nav_item>Projects <img src=https://arthurzucker.github.io/icons/caret-icon.svg alt=icon class=nav_icon></a>
<div class=nav_sub>
<span class=nav_child></span>
<a href=https://arthurzucker.github.io/projects/flying_foxes class="nav_child nav_item">Flying Foxes</a>
<a href=https://arthurzucker.github.io/projects/polytech_projects class="nav_child nav_item">School projects</a>
</div>
</div>
<div class=nav_parent>
<a href=https://arthurzucker.github.io/notes/ class=nav_item>Notes <img src=https://arthurzucker.github.io/icons/caret-icon.svg alt=icon class=nav_icon></a>
<div class=nav_sub>
<span class=nav_child></span>
<a href=https://arthurzucker.github.io/notes/rl class="nav_child nav_item">Reinforcement Learning</a>
<a href=https://arthurzucker.github.io/notes/recvis class="nav_child nav_item">Recvis</a>
</div>
</div>
<div class=nav_parent>
<a href=https://arthurzucker.github.io/about/ class=nav_item>Author </a>
</div>
<div class=nav_parent>
<a href=https://arthurzucker.github.io/links/ class=nav_item>Links <img src=https://arthurzucker.github.io/icons/caret-icon.svg alt=icon class=nav_icon></a>
<div class=nav_sub>
<span class=nav_child></span>
<a href=https://www.polytech.sorbonne-universite.fr/ class="nav_child nav_item">Polytech</a>
<a href=https://www.batresearch.net/ class="nav_child nav_item">IRGB</a>
</div>
</div>
<div class=nav_parent>
<a href=https://arthurzucker.github.io/search/ class=nav_item><img src=/icons/search.svg style=height:1.2rem;display:inline;margin:0> </a>
</div>
<div class=follow>
<a href=https://github.com/ArthurZucker><svg class="icon"><use xlink:href="#github"/></svg>
</a>
<a href=https://www.linkedin.com/in/arthur-zucker-8a0445144/><svg class="icon"><use xlink:href="#linkedin"/></svg>
</a>
<div class=color_mode>
<input type=checkbox class=color_choice id=mode>
<label for=mode style=display:none>Color mode</label>
</div>
</div>
</div>
</nav>
</header>
<main>
<div class="wrap content grid-inverse">
<article class=post_content>
<h1 class=post_title>[MVA] RL : Reinforcement Learning, how to model an RL problem: Markov Decision Processes</h1><div class=post_meta><svg class="icon"><use xlink:href="#calendar"/></svg>
<span class=post_date>
Sep 22, 2021</span>
<a href=https://arthurzucker.github.io/tags/reinforcement-learning class="post_tag button button_translucent">Reinforcement Learning
</a><svg class="icon"><use xlink:href="#loop"/></svg>
<span class=post_edited_date>
Oct 12, 2021</span>
</div>
<div class=post_share>
Share on:
<a href="https://twitter.com/intent/tweet?text=%5bMVA%5d%20RL%20%3a%20Reinforcement%20Learning%2c%20how%20to%20model%20an%20RL%20problem%3a%20Markov%20Decision%20Processes&url=https%3a%2f%2farthurzucker.github.io%2fnotes%2frl%2flecture1%2f&tw_p=tweetbutton" class=twitter title="Share on Twitter" target=_blank rel=nofollow><svg class="icon"><use xlink:href="#twitter"/></svg>
</a>
<a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2farthurzucker.github.io%2fnotes%2frl%2flecture1%2f&t=%5bMVA%5d%20RL%20%3a%20Reinforcement%20Learning%2c%20how%20to%20model%20an%20RL%20problem%3a%20Markov%20Decision%20Processes" class=facebook title="Share on Facebook" target=_blank rel=nofollow><svg class="icon"><use xlink:href="#facebook"/></svg>
</a>
<a href=#linkedinshare id=linkedinshare class=linkedin title="Share on LinkedIn" rel=nofollow><svg class="icon"><use xlink:href="#linkedin"/></svg>
</a>
<a href=https://arthurzucker.github.io/notes/rl/lecture1/ title="Copy Link" class="link link_yank"><svg class="icon"><use xlink:href="#copy"/></svg>
</a>
</div>
<div class=js-toc-content><style>r{color:Red}o{color:Orange}g{color:Green}b{color:Blue}</style>
<p>The course is provided by Matteo Pirotta a Facebook AI Researcher, and is based on one of the previous MVA RL classes by Alessandro Lazaric (FAIR). @TODO add links to their linkdin or websites</p>
<h2 id=introduction>Introduction</h2>
<h3 id=what-is-rl-when-to-use-it-why-rl>What is RL? When to use it? Why RL?</h3>
<p>RL, a short for Reinforcement Learning, is a subclass of machine learning, which focuses on building a model able to learn from his action in an environnement. It builds its knowledge threw <em>reinforcing</em> what it knows, threw trial and error.</p>
<p>RL has a lot of applications, ranging from finance (where the goal can be to optimize sales live, and the model learns to buy/sell) to robotics, game solving or exploration. Any task which requires an action to be taken to maximize a utility function can use RL models. One of the most successful usage of RL is <strong><a href>alphago</a></strong>, a model which taught himself how to play <em>Go</em>, and beat the world champion. <em>Go</em> was considered to be the last barrier, one of the most complex game, given the huge amount of configurations : more than <strong>??</strong> which is more atom than the universe contains.</p>
<p>Let's try to give a proper definition to RL.
In any RL model, there is :</p>
<ul>
<li>an RL agent</li>
<li>an Environment, deterministic or stochastic</li>
<li>actions</li>
<li>rewards, sometimes long-term like in go, or short term in portfolio management. But you can have intermediate rewards/signals. You need the knowledge to do that, but might not be possible. Long time effects/certain configurations are optimal?. The function is still free to be designed.</li>
<li>states, sometimes unknown, like the stock prices that are unpredictable. Can only see a realization. Lots of uncertainty.</li>
</ul>
<p>Goal is to maximize the reward. Sometimes you don't know the Environment / reward and can only observe! Connected to adaptive control theory, act by looking at the observations you get through the interactions with the systems.</p>
<p><strong>Framework for learning by interaction under uncertainty</strong>, agent takes action, receives reward, and the environment evolves to the next state.</p>
<h3 id=content-of-this-note->Content of this note :</h3>
<ul>
<li>Reward : function?</li>
<li>Value : cumulative reward an agent can take</li>
<li>Policy : Actions to take in a state</li>
</ul>
<p>How to model and RL problem?</p>
<ul>
<li>Markov process</li>
</ul>
<p>How to solve it?</p>
<ul>
<li>Dynamic programming</li>
</ul>
<p>Solve incrementally?</p>
<ul>
<li>temporal difference, Q-learning using stochastic approximation.</li>
</ul>
<p>How to solve approximately an RL problem?</p>
<ul>
<li>policy gradient, TD based method, deep RL</li>
</ul>
<h2 id=1-markov-decision-process>1. Markov Decision Process</h2>
<p>A RL problem is modelled using <em>Markov processes</em>.</p>
<h3 id=the-reinforcement-learning-model>The Reinforcement Learning Model</h3>
<p>(insert image provide)</p>
<p>It can be described using simple keywords :</p>
<ul>
<li>Environment, which defines where the Agent evolves</li>
<li>Agent, is the ??? learning</li>
<li>Actions and their associated rewards</li>
<li>States, which allow the definition of the system</li>
</ul>
<p>Lets follow along a simple example throughout this course, in order to better understand each notions.
TODO, define the 4 notions above in the case of a simple atari game.</p>
<h3 id=the-rl-interactions>The RL interactions</h3>
<p>The classic RL interaction is as follows:</p>
<p>We have a discrete decision time $t$ (which would be for example seconds), and at each timestamp :</p>
<ol>
<li>The <em>agent</em> selects an <em>action</em> $a_t$, based on $s_t$ the current <em>state</em> of the system.</li>
<li>The <em>agent</em> gets his <em>reward</em> $r_t$</li>
<li>The <em>Environment</em> moves to a new state $s_{t+1}$</li>
</ol>
<h3 id=markov-chains>Markov chains</h3>
<p>Markov Chains are the building blocks of RL. A Markov Chain is a stochastic (which means that it includes randomness ) model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. In order to create a model, we need to introduce mathematical definitions.</p>
<p><g>Definition :</g> Given $S$, a state space and a bounded compact subset of the Euclidean space, the discrete time dynamic system $(s_t)_{t\in \mathbb{N}}$ is a markov Chain <strong>if it satisfies</strong> the <r>Markov property</r>:</p>
<p>$$ \mathbb{P}\left(s_{t+1}=s \mid s_{t}, s_{t-1}, \ldots, s_{0}\right)=\mathbb{P}\left(s_{t+1}=s \mid s_{t}\right)$$</p>
<p>This means that given an initial state $s_0$, the markov chain is defined by the transition probability : $p\left(s^{\prime} \mid s\right)=\mathbb{P}\left(s_{t+1}=s^{\prime} \mid s_{t}=s\right)$.</p>
<p><o> Intuition:</o> a markov chain only needs an initial state, and a transition probability to figure out any later state, it is defined by the transition probability.</p>
<p>@TODO add 4 states markov chain example with attari like states</p>
<h3 id=markov-decision-process-mdp>Markov Decision Process (MDP)</h3>
<p>A Markov Decision Process is a tool, to model decision making when outcomes can be random.</p>
<p><g>Definition :</g> a Markov Decision Process is defined as a tuple <b>$M=(S,A,p,r)$</b> with</p>
<ul>
<li>$S$ the (finite) State space</li>
<li>$A$ the (finite) Action space</li>
<li>$p(s'|s,a)$ the transition probability, such that $\mathbb{P}\left(s_{t+1}=s' \mid s_{t}=s,a_t=a\right)$</li>
<li>$r(s,a,s')$ is the reward of the transition from $s$ to $s'$ taking the action $a$. The reward can be stochastic, but we can express its distribution, $v$ for $s,a$ as $v(s,a)$. Thus the reward can be writtent as : $r(s,a) = \mathbb{E}_{R\sim v(s,a)}[R]$.</li>
</ul>
<h4 id=assumptions>Assumptions</h4>
<p><b>Markov assumptions</b> : the current state $s$ and the action $a$ are a sufficient statistic for the next state $s'$. This means that "no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter"<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Our statistic provides as much information as possible.</p>
<p><b>Time assumption</b> : time is discrete, not continuous. At each step, $t\rightarrow t+1$.</p>
<p><b>Reward assumption</b> : the reward is uniquely defined by a transition from a state $s$, to $s'$ and an action $a$.</p>
<p><b>Stationary assumption</b> : transition and reward don't change with time. Otherwise, it's non-stationary.</p>
<h2 id=2-policy>2. Policy</h2>
<p><g>Definition : </g>a decision rule $d$, which chooses an action from the state $S$ of the system can be :</p>
<ul>
<li>Deterministic, given the same set of state $S$, the action $a\in A$ will be chosen</li>
<li>Stochastic, a probability distribution over the actions determines $a$.</li>
<li>History-dependant, and is thus</li>
<li>Markov</li>
</ul>
<p><g>Definition</g> : A policy is a sequence of decision rules, it can be :</p>
<ul>
<li>Non-stationary, then $\pi = (d_0,d_1,...)$</li>
<li>stationary, then $\pi = (d,d,...)$</li>
</ul>
<p>At each round t, an agent following the policy $\pi$ selects the actions $a_t \sim d_t(s_t)$ .</p>
<h3 id=markov-chain-of-a-policy>Markov chain of a policy</h3>
<p>A stationary policy defines a Markov chain on a random process! Why? Well the transition probability of a random process $(s_t)_{t \in \mathbb{N}}$ with regards to a stationary policy $\pi$ is the following :</p>
<p>$$ P^\pi (s'\mid s) = \mathbb{P}(s_{t+1}=s' \mid s_t = s, \pi) $$</p>
<p>Every actions in $A$ (even if it does not take the state from $s$ to $s'$) has to be taken into account. Thus we deduce that $$ p^\pi (s'\mid s) = \sum_{a\in A} \pi(s,a)p(s'\mid s,a) $$</p>
<h2 id=3-optimality-principle>3. Optimality Principle</h2>
<p>This part answers the question of how good is a policy, and how do we measure this.</p>
<h3 id=state-value-function>State value function</h3>
<p>The state value function allows us to determine the effectiveness of a policy. Various definitions can be used depending on the problem at hand. For the following definitions, let $\pi = (d_1, d_2, ...,)$ be a deterministic policy.</p>
<div style=padding-left:30px>
üé≤ The <g>Finite time horizon T</g> policy, sets a deadline $T$, at which the sum of the rewards up to $T$ will be computed. The value function can be expressed as :
<p>$$ V^\pi(t,s) = \mathbb{E}\left[ \sum_{\tau = t}^{T-1} r(s_\tau,d_\tau(h_\tau)) + R(s_T) \mid s_t = s; \pi = (d_1, d_2, ...,) \right ]$$</p>
<p>Here, where $R$ is a value function for the final state. This mathematical formula simply expresses the fact that the state value function $V$, for a certain policy $\pi$, a time $t$ which will be the the "beginning" of the computation, and a state $s$ such that at time $t$, the system is in the state $s$, is equal to the Expectation of the sum of the rewards obtained taking actions according ot the policy, and receiving the corresponding rewards. We have to take into account the Expectation because the decision process can be stochastic, and thus the Expectation gives the equivalent of the mean rewards in a stochastic model.<br>
$\qquad$üìù It is usually used when there is a deadline to meet.</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Infinite Time Horizon with discount </g> translates a problem that never terminates. Rewards closer in time receive a higher importance :
<p>$$ V^{\pi}\left(s\right)\ =\ \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^tr\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi \right] $$</p>
<p>where $0\leq\gamma&lt;1$ is the <em>discount</em> factor. If it's small, the value function focusses on short-term reward, if it's big, on long term reward.
This series always converges <br>
$\qquad$üìù We implement it when there is uncertainty about the deadline, or intrinsic definition of discount ?????? @TODO give examples</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Stochastic shortest path </g>, the problem never terminates but the agent will reach a termination state.
<p>$$V^{\pi}\left(s\right)\ =\ \mathbb{E}\left[\sum_{t=0}^{T_{\pi}}r\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi\right]$$</p>
<p>$T_{\pi}$ : first random time when the termination state is achieved. <br>
$\qquad$üìù Used when there is a specific goal condition</p>
</div>
<div style=padding-left:30px>
üé≤ The <g> Infinite time horizon with average reward</g>, the problem never terminates but the agent focuses on the expected average of the rewards
<p>$$V^{\pi}\left(s\right)\ =\ \lim_{T\to\infty}\ \mathbb{E}\left[\frac{1}{T}\sum_{t=0}^{T-1}r\left(s_t,d_t\left(h_t\right)\right)\ \mid s_{0\ }=s;\pi\right]$$</p>
<p>$T_{\pi}$ : first random time when the termination state is achieved. <br>
$\qquad$üìù Used when the system should be constantly controlled over tim</p>
</div>
<span class=expand>
<span class=expand-label style=cursor:pointer onclick="$h=$(this),$h.next('div').slideToggle(100,function(){$h.children('i').attr('class',function(){return $h.next('div').is(':visible')?'closeme':'fas fa-fas fa-bars'})})"><svg class="icon"><use xlink:href="#loop"/></svg>
Click me!<svg class="icon"><use xlink:href="#.Get%201"/></svg>
</span>
<div class=expand-content style="display:none;margin:10px 0">
Hidden content
</div>
</span>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p><a href=https://en.wikipedia.org/wiki/Markov_chain>https://en.wikipedia.org/wiki/Markov_chain</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p><a href=https://en.wikipedia.org/wiki/Sufficient_statistic#cite_note-Fisher1922-1>https://en.wikipedia.org/wiki/Sufficient_statistic#cite_note-Fisher1922-1</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
</article>
<aside class=sidebar>
<section class=sidebar_inner>
<div class=sticky-toc>
<div class=js-toc></div>
</div>
</section>
</aside>
</div>
</main><svg width="0" height="0" class="hidden"><symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook"><path d="M437 0H75C33.648.0.0 33.648.0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352.0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter"><path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68.0 01-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043.0-1.924.366-2.643 1.078A3.56 3.56.0 008.766 5.383c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846.0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47.0.929.273 1.705.82 2.388a3.623 3.623.0 002.115 1.291c-.312.08-.641.118-.979.118-.312.0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652.0 002.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422.0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139.0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77.0 001.172-4.892v-.468a7.788 7.788.0 001.84-1.921 8.142 8.142.0 01-2.11.593z"/></symbol><symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V4e2c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5.0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar"><path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916.0 1e2v352c0 33.084 26.916 60 60 60h392c33.084.0 60-26.916 60-60V1e2c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028.0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028.0 20 8.972 20 20v48z"/><path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github"><path d="M255.968 5.329C114.624 5.329.0 120.401.0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384.0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008.0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992.0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584.0 34.368-.32 62.08-.32 70.496.0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"/></symbol><symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss"><circle cx="3.429" cy="20.571" r="3.429"/><path d="M11.429 24h4.57C15.999 15.179 8.821 8.001.0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"/><path d="M24 24C24 10.766 13.234.0.0.0v4.571c10.714.0 19.43 8.714 19.43 19.429z"/></symbol><symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin"><path d="M437 0H75C33.648.0.0 33.648.0 75v362c0 41.352 33.648 75 75 75h362c41.352.0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="arrow"><path d="M604.501 440.509 325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298.0 36.323s26.223 10.024 36.222.0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221.0 9.999-10.023 9.999-26.298.0-36.323z"/></symbol><symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly"><path d="M504.971 239.029 448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255.0-24 10.745-24 24s10.745 24 24 24h44c19.851.0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002.0 004e2 320v108c0 19.851-16.149 36-36 36h-44c-13.255.0-24 10.745-24 24s10.745 24 24 24h44c46.318.0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568.0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255.0 24-10.745 24-24S205.255.0 192 0h-44c-46.318.0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568.0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255.0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851.0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002.0 00112 192z"/></symbol><symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="copy"><path d="M23 2.75A2.75 2.75.0 0020.25.0H8.75A2.75 2.75.0 006 2.75v13.5A2.75 2.75.0 008.75 19h11.5A2.75 2.75.0 0023 16.25zM18.25 14.5h-7.5a.75.75.0 010-1.5h7.5a.75.75.0 010 1.5zm0-3h-7.5a.75.75.0 010-1.5h7.5a.75.75.0 010 1.5zm0-3h-7.5a.75.75.0 010-1.5h7.5a.75.75.0 010 1.5z"/><path d="M8.75 20.5A4.255 4.255.0 014.5 16.25V2.75c0-.086.02-.166.025-.25H3.75A2.752 2.752.0 001 5.25v16A2.752 2.752.0 003.75 24h12a2.752 2.752.0 002.75-2.75v-.75z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme"><path d="M284.286 256.002 506.143 34.144c7.811-7.811 7.811-20.475.0-28.285-7.811-7.81-20.475-7.811-28.285.0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285.0-7.81 7.811-7.811 20.475.0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475.0 28.285a19.938 19.938.0 0014.143 5.857 19.94 19.94.0 0014.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475.0-28.285L284.286 256.002z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu"><path d="M492 236H20c-11.046.0-20 8.954-20 20s8.954 20 20 20h472c11.046.0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954.0 96s8.954 20 20 20h472c11.046.0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046.0-20 8.954-20 20s8.954 20 20 20h472c11.046.0 20-8.954 20-20s-8.954-20-20-20z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram"><path d="M12 2.163c3.204.0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849.0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204.0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849.0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zM12 0C8.741.0 8.333.014 7.053.072c-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948s.014 3.668.072 4.948c.2 4.358 2.618 6.78 6.98 6.98C8.333 23.986 8.741 24 12 24s3.668-.014 4.948-.072c4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948s-.014-3.667-.072-4.947c-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403.0-6.162 2.759-6.162 6.162S8.597 18.163 12 18.163s6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zM12 16c-2.209.0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796.0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795.0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="youtube"><path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23.0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23.0C23.512 20.55 23.971 18.196 24 12c-.029-6.185-.484-8.549-4.385-8.816zM9 16V8l8 3.993L9 16z"/></symbol><symbol xmlns="http://www.w3.org/2000/svg" id="loop" viewBox="0 0 32 32"><path d="M27.802 5.197C24.877 2.003 20.672.0 15.999.0c-8.837.0-16 7.163-16 16h3c0-7.18 5.82-13 13-13 3.844.0 7.298 1.669 9.678 4.322L20.999 12h11V1l-4.198 4.197z"/><path d="M29 16c0 7.18-5.82 13-13 13-3.844.0-7.298-1.669-9.678-4.322L11 20H0v11l4.197-4.197C7.122 29.997 11.327 32 16 32c8.837.0 16-7.163 16-16h-3z"/></symbol></svg>
<footer class=footer>
<div class="footer_inner wrap pale">
<img alt="LIFE AS A YOUNG ENGINEER" src=https://arthurzucker.github.io/icons/apple-touch-icon.png class="icon icon_2 transparent">
<p>Copyright &copy;&nbsp;<span class=year>2021</span>&nbsp;Life as a young engineer. All Rights Reserved</p><a class=to_top href=#documentTop><svg class="icon"><use xlink:href="#arrow"/></svg>
</a>
</div>
</footer>
<script>window.MathJax={tex:{inlineMath:[['$','$']]},options:{enableMenu:!0}}</script>
<script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
<style>mjx-container.MathJax[display=inline]{font-size:150%!important}mjx-container.MathJax[display=true]{text-align:left!important}@media screen and (min-width:600px){mjx-container.MathJax[display=true]{padding-left:2rem}}</style>
<script defer src=/tocbot/tocbot.min.js></script>
<script>function docReady(a){document.readyState==="complete"||document.readyState==="interactive"?setTimeout(a,1):document.addEventListener("DOMContentLoaded",a)}docReady(function(){var b=document.getElementsByTagName("head")[0],c;function a(a){window.tocbot.init({tocSelector:'.js-toc',contentSelector:'.js-toc-content',headingSelector:'h1, h2, h3'})}a()})</script>
<script type=text/javascript src=https://arthurzucker.github.io/js/bundle.min.fad5deef9fb8788694f791bf960a39ad1fa85ea9930819f679df19a20785d9d47d3dccc519e3848753179c86f3f6fe36d06f267539ab55d4104e92fdd99d6962.js integrity="sha512-+tXe75+4eIaU95G/lgo5rR+oXqmTCBn2ed8ZogeF2dR9PczFGeOEh1MXnIbz9v420G8mdTmrVdQQTpL92Z1pYg==" crossorigin=anonymous></script>
</body>
</html>