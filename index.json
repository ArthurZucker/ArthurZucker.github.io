[{"categories":["MOOC"],"contents":" r { color: Red } o { color: Orange } g { color: Green } b { color: Blue }  The course is provided by Matteo Pirotta a Facebook AI Researcher, and is based on one of the previous MVA RL classes by Alessandro Lazaric (FAIR). @TODO add links to their linkdin or websites\nIntroduction What is RL? When to use it? Why RL? RL, a short for Reinforcement Learning, is a subclass of machine learning, which focuses on building a model able to learn from his action in an environnement. It builds its knowledge threw reinforcing what it knows, threw trial and error.\nRL has a lot of applications, ranging from finance (where the goal can be to optimize sales live, and the model learns to buy/sell) to robotics, game solving or exploration. Any task which requires an action to be taken to maximize a utility function can use RL models. One of the most successful usage of RL is alphago, a model which taught himself how to play Go, and beat the world champion. Go was considered to be the last barrier, one of the most complex game, given the huge amount of configurations : more than ?? which is more atom than the universe contains.\nLet's try to give a proper definition to RL. In any RL model, there is :\n an RL agent an Environment, deterministic or stochastic actions rewards, sometimes long-term like in go, or short term in portfolio management. But you can have intermediate rewards/signals. You need the knowledge to do that, but might not be possible. Long time effects/certain configurations are optimal?. The function is still free to be designed. states, sometimes unknown, like the stock prices that are unpredictable. Can only see a realization. Lots of uncertainty.  Goal is to maximize the reward. Sometimes you don't know the Environment / reward and can only observe! Connected to adaptive control theory, act by looking at the observations you get through the interactions with the systems.\nFramework for learning by interaction under uncertainty, agent takes action, receives reward, and the environment evolves to the next state.\nContent of this note :  Reward : function? Value : cumulative reward an agent can take Policy : Actions to take in a state  How to model and RL problem?\n Markov process  How to solve it?\n Dynamic programming  Solve incrementally?\n temporal difference, Q-learning using stochastic approximation.  How to solve approximately an RL problem?\n policy gradient, TD based method, deep RL  1. Markov Decision Process A RL problem is modelled using Markov processes.\nThe Reinforcement Learning Model (insert image provide)\nIt can be described using simple keywords :\n Environment, which defines where the Agent evolves Agent, is the ??? learning Actions and their associated rewards States, which allow the definition of the system  Lets follow along a simple example throughout this course, in order to better understand each notions. TODO, define the 4 notions above in the case of a simple atari game.\nThe RL interactions The classic RL interaction is as follows:\nWe have a discrete decision time $t$ (which would be for example seconds), and at each timestamp :\n The agent selects an action $a_t$, based on $s_t$ the current state of the system. The agent gets his reward $r_t$ The Environment moves to a new state $s_{t+1}$  Markov chains Markov Chains are the building blocks of RL. A Markov Chain is a stochastic (which means that it includes randomness ) model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event1. In order to create a model, we need to introduce mathematical definitions.\nDefinition : Given $S$, a state space and a bounded compact subset of the Euclidean space, the discrete time dynamic system $(s_t)_{t\\in \\mathbb{N}}$ is a markov Chain if it satisfies the Markov property:\n$$ \\mathbb{P}\\left(s_{t+1}=s \\mid s_{t}, s_{t-1}, \\ldots, s_{0}\\right)=\\mathbb{P}\\left(s_{t+1}=s \\mid s_{t}\\right)$$\nThis means that given an initial state $s_0$, the markov chain is defined by the transition probability : $p\\left(s^{\\prime} \\mid s\\right)=\\mathbb{P}\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s\\right)$.\n Intuition: a markov chain only needs an initial state, and a transition probability to figure out any later state, it is defined by the transition probability.\n@TODO add 4 states markov chain example with attari like states\nMarkov Decision Process (MDP) A Markov Decision Process is a tool, to model decision making when outcomes can be random.\nDefinition : a Markov Decision Process is defined as a tuple $M=(S,A,p,r)$ with\n $S$ the (finite) State space $A$ the (finite) Action space $p(s'|s,a)$ the transition probability, such that $\\mathbb{P}\\left(s_{t+1}=s' \\mid s_{t}=s,a_t=a\\right)$ $r(s,a,s')$ is the reward of the transition from $s$ to $s'$ taking the action $a$. The reward can be stochastic, but we can express its distribution, $v$ for $s,a$ as $v(s,a)$. Thus the reward can be writtent as : $r(s,a) = \\mathbb{E}_{R\\sim v(s,a)}[R]$.  Assumptions Markov assumptions : the current state $s$ and the action $a$ are a sufficient statistic for the next state $s'$. This means that \u0026quot;no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter\u0026quot;2. Our statistic provides as much information as possible.\nTime assumption : time is discrete, not continuous. At each step, $t\\rightarrow t+1$.\nReward assumption : the reward is uniquely defined by a transition from a state $s$, to $s'$ and an action $a$.\nStationary assumption : transition and reward don't change with time. Otherwise, it's non-stationary.\n2. Policy Definition : a decision rule $d$, which chooses an action from the state $S$ of the system can be :\n Deterministic, given the same set of state $S$, the action $a\\in A$ will be chosen Stochastic, a probability distribution over the actions determines $a$. History-dependant, and is thus Markov  Definition : A policy is a sequence of decision rules, it can be :\n Non-stationary, then $\\pi = (d_0,d_1,...)$ stationary, then $\\pi = (d,d,...)$  At each round t, an agent following the policy $\\pi$ selects the actions $a_t \\sim d_t(s_t)$ .\nMarkov chain of a policy A stationary policy defines a Markov chain on a random process! Why? Well the transition probability of a random process $(s_t)_{t \\in \\mathbb{N}}$ with regards to a stationary policy $\\pi$ is the following :\n$$ P^\\pi (s'\\mid s) = \\mathbb{P}(s_{t+1}=s' \\mid s_t = s, \\pi) $$\nEvery actions in $A$ (even if it does not take the state from $s$ to $s'$) has to be taken into account. Thus we deduce that $$ p^\\pi (s'\\mid s) = \\sum_{a\\in A} \\pi(s,a)p(s'\\mid s,a) $$\n3. Optimality Principle This part answers the question of how good is a policy, and how do we measure this.\nState value function The state value function allows us to determine the effectiveness of a policy. Various definitions can be used depending on the problem at hand. For the following definitions, let $\\pi = (d_1, d_2, ...,)$ be a deterministic policy.\nüé≤ The Finite time horizon T policy, sets a deadline $T$, at which the sum of the rewards up to $T$ will be computed. The value function can be expressed as : $$ V^\\pi(t,s) = \\mathbb{E}\\left[ \\sum_{\\tau = t}^{T-1} r(s_\\tau,d_\\tau(h_\\tau)) + R(s_T) \\mid s_t = s; \\pi = (d_1, d_2, ...,) \\right ]$$\nHere, where $R$ is a value function for the final state. This mathematical formula simply expresses the fact that the state value function $V$, for a certain policy $\\pi$, a time $t$ which will be the the \u0026quot;beginning\u0026quot; of the computation, and a state $s$ such that at time $t$, the system is in the state $s$, is equal to the Expectation of the sum of the rewards obtained taking actions according ot the policy, and receiving the corresponding rewards. We have to take into account the Expectation because the decision process can be stochastic, and thus the Expectation gives the equivalent of the mean rewards in a stochastic model.\n$\\qquad$üìù It is usually used when there is a deadline to meet.\n üé≤ The  Infinite Time Horizon with discount  translates a problem that never terminates. Rewards closer in time receive a higher importance : $$ V^{\\pi}\\left(s\\right)\\ =\\ \\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^tr\\left(s_t,d_t\\left(h_t\\right)\\right)\\ \\mid s_{0\\ }=s;\\pi \\right] $$\nwhere $0\\leq\\gamma\u0026lt;1$ is the discount factor. If it's small, the value function focusses on short-term reward, if it's big, on long term reward. This series always converges $\\qquad$üìù We implement it when there is uncertainty about the deadline, or intrinsic definition of discount ?????? @TODO give examples\n üé≤ The  Stochastic shortest path , the problem never terminates but the agent will reach a termination state. $$V^{\\pi}\\left(s\\right)\\ =\\ \\mathbb{E}\\left[\\sum_{t=0}^{T_{\\pi}}r\\left(s_t,d_t\\left(h_t\\right)\\right)\\ \\mid s_{0\\ }=s;\\pi\\right]$$\n$T_{\\pi}$ : first random time when the termination state is achieved. $\\qquad$üìù Used when there is a specific goal condition\n üé≤ The  Infinite time horizon with average reward, the problem never terminates but the agent focuses on the expected average of the rewards $$V^{\\pi}\\left(s\\right)\\ =\\ \\lim_{T\\to\\infty}\\ \\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=0}^{T-1}r\\left(s_t,d_t\\left(h_t\\right)\\right)\\ \\mid s_{0\\ }=s;\\pi\\right]$$\n$T_{\\pi}$ : first random time when the termination state is achieved. $\\qquad$üìù Used when the system should be constantly controlled over tim\n   Click me!    Hidden content   üìù\nmatrix notation. Fundamental matrix $(I-\\gamma P ^\\pi )^-1 \\times r^\\pi$ = solve it by inverting the SxS matrix. ### Control? MDP given, find the optimal policy policy evaluation is used for that! = to find the optimal policy. Bellman equation again, principle of optimality [1957]. P optimal if the next states are also optimal. (hints on the fixed point equation). Now deterministic, always there in MDP! Given the equations, the optimal policy maximizes the value function. Might be multiple action, pic any. 2 important incomes : bellman inequality, fixed point; optimal V* = get policy by being greedy. No linear system, so can't solve it linearly. Complication starts here. Iterative and all. The control problem is what we need to solve! Assumption? we know P and R, and that the MDP is stable. #### Bellman operator associated to the bellman equation. Can right now using the operator : $V= T^\\pi V$ same for optimal **Properies** : - Monotonicy - Additivity - contraction in Linf norm. Application of the operator, makes the results closer than were we started. Its converging toward something. - fixed point : exists a unique fixed point $V^\\pi$ and $V^*$ Results from Banach fixed point theorem. The iterative operator will converge! Key property! To define a solution.  Summarize : Few concepts to remind : - transition proba - reward function - policy define a MDP Optimality? aime ot solve the cumulative reward = V or value function Can restrict the set of policies, stationary, markov and deterministic, which simplifies. Bellman operators are fundamental and relates the definition in a single step. Can right as a single step, value is reward plus expectation in the next step. Nice properties, guarantees that we can run it iteratively. page 30 : pi * : a policie, the optimal is non zero only for the actions that are optimal. Select any deterministic policy and selecting it. pi^* (s) belongs to the argmax. = its deterministic and can thus pic any actions that belong to the argmax. Greedy! with respect to the bellman operators. p30 strictly positive **State action** vs **state value** function. Another element? State action greedy function? utility vs utility and action. Sum of rewards discounted, forcing initial action to be a, then follow the policy for all the next states etc. Q function?! Optimal policy is greedy with respect to Q. What is greedy? p41, V*(s'). Start from Q* is simply pi(s) \\in max Q*(s,a)) They also satisfy the bellman equation. --   https://en.wikipedia.org/wiki/Markov_chain\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://en.wikipedia.org/wiki/Sufficient_statistic#cite_note-Fisher1922-1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://arthurzucker.github.io/notes/rl/lecture1/","tags":["Reinforcement Learning"],"title":"[MVA] RL : Reinforcement Learning, how to model an RL problem: Markov Decision Processes"},{"categories":["Intership project"],"contents":"During my 3rd year internship at Chainos Solution, my colleague Younes Belkda and I developed an automatic Table structure recognition based on Artificial intelligence. We called it CLUSTI!\nProblem statement Some organizations still use huge amounts of scanned tables / invoices. Rather than manually typing the contents of the tables into an excel file, one can think of using Deep Learning to tackle this problem. Once you identify the distinguishable cells on the tables, we can pass an OCR (Optical Character Recognition) to each cell to read its contents and convert it to an excel file for example.\nThe method performs table structure recognition in an unsupervised way\nHere comes the task of table structure recognition, the main goal would be to efficiently detect the individual cells given an image containing textual information (characters, digits).\nThis work has been achieved through one of my internships together with Arthur Zucker, and the work resulted on the publication of a research article that can be browsed here.\nWhy this task is so challenging ? This task can be tackled from several directions, if we assume that the tables are always perfect and clean, a simple computer vision approach (line / edge / corner detections algorithms) would be sufficient. However, the tables that have been given by our clients are hard to process and doesn't follow the same pattern in terms of global structure.\nIn this case, Deep Learning is the best fit to tackle this problem. But How ? For example, we can think of an object detection algorithm that predicts the cell regions or the ruling lines of the input table. But getting a labeled dataset for this task makes thing very complicated, and even if we had access to such dataset, there is no guarantee that the model will perform well in practice. What if the table contains unseen characters from a different alphabet, etc.\nHere comes the well-discussed border that lies between the research field and industry. In research most of the benchmark datasets are clean and sometimes, ML models trained and evaluated on some specific datasets are hard to deploy in real-world application.\nFortunately, there is a high excitement around text detection algorithms using Deep Learning in the litterature and we can take advantage of it. When we were working at this project, CRAFT algorithm was the state-of-the-art text detection algorithm.\nPipeline of CRAFT\nSince these models are state-of-the-art models we maximize the chances that at test time, it will handle well the corner cases. In practice, this model worked well on scanned invoices so we have decided to stick with it. As stated before, CRAFT was the state-of-the-art at the time we were working on the project, this method can be implmented using current state-of-the-art models.\nWhat is next ? The idea of the paper is to deploy a hierarchical AI-based solution for that. If you are curious about the method and know more about it, please visit this website and request for a full version of the paper.\n","permalink":"https://arthurzucker.github.io/automatic-table-structure-recogniton/","tags":["Computer vision","Clustering"],"title":"Automatic Table Structure Recogniton"},{"categories":["AI Projects"],"contents":"BRATS (BRAin Tumor Segmentation) is a kaggle competition held every year, which relies on the participation of teams to solve mostly semantic segmentation task related to medical image analysis. This year, the goal of the challenge was to determine the presence of a specific genetic sequence, MGMT promoter methylation, in a tumor, using MRI (Magnetic Resonance Image). This gene sequence plays a strong role in the outcome of chemotherapy in patients with brain tumor, but it can only be analysed using DNA extraction, which takes a lot of time. If a MRI is enough to assess the possibility of giving chemotherapy, it could save thousands of lives.\nI am working on the challenge with my long time colleague, Younes Belkada, who described a part of our work in his blog post.\n","permalink":"https://arthurzucker.github.io/posts/ai-projects/kaggle/brats/","tags":["Computer Vision","Healthcare"],"title":"BRATS2021, Brain Tumor Segmentation"}]